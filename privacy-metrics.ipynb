{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample datasets containing hypothetical medical records; features include age, last three digits of pincode and BP; Target feature is medical condition\n",
    "dataset1 = pd.DataFrame({\n",
    "    'age': [25, 25, 25, 32, 32, 45, 45, 45, 58, 58],\n",
    "    'pincode': ['123', '123', '123', '234', '234', '345', '345', '345', '456', '456'],\n",
    "    'blood_pressure': ['high', 'normal', 'high', 'low', 'normal', 'high', 'high', 'normal', 'low', 'high'],\n",
    "    'condition': ['diabetes', 'flu', 'hypertension', 'asthma', 'diabetes', \n",
    "                 'flu', 'diabetes', 'asthma', 'hypertension', 'flu']\n",
    "})\n",
    "\n",
    "dataset2 = pd.DataFrame({\n",
    "    'age': [25, 25, 25, 32, 32, 32, 45, 45, 58, 58],\n",
    "    'pincode': ['123', '123', '123', '234', '234', '234', '345', '345', '456', '456'],\n",
    "    'blood_pressure': ['high', 'high', 'normal', 'low', 'low', 'normal', 'high', 'normal', 'low', 'high'],\n",
    "    'condition': ['diabetes', 'diabetes', 'flu', 'asthma', 'asthma', 'flu', \n",
    "                 'hypertension', 'diabetes', 'flu', 'hypertension']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Privacy Metrics class with functions to calculate individual scores and a compound score to compare datasets\n",
    "class PrivacyMetrics:\n",
    "    def __init__(self, epsilon=0.1):\n",
    "        self.epsilon = epsilon #differential privacy parameter, set to 0.1 by default\n",
    "    \n",
    "    def k_anonymity(self, data: pd.DataFrame, quasi_identifiers: List[str]) -> int:        \n",
    "        #Group by quasi-identifiers and count group sizes; minimum group size gives k-anonymity score\n",
    "        grouped = data.groupby(quasi_identifiers).size()\n",
    "        k = grouped.min()\n",
    "        return k\n",
    "\n",
    "    def l_diversity(self, data: pd.DataFrame, quasi_identifiers: List[str], sensitive_attribute: str) -> float:\n",
    "        #number of distinct values per group gives l-diversity score; Constraint: l <= k\n",
    "        min_diversity = float('inf')\n",
    "        groups = data.groupby(quasi_identifiers)        \n",
    "        for _, group in groups:\n",
    "            distinct_values = len(group[sensitive_attribute].unique()) \n",
    "            min_diversity = min(min_diversity, distinct_values)        \n",
    "        return min_diversity\n",
    "\n",
    "    def t_closeness(self, data: pd.DataFrame, quasi_identifiers: List[str], sensitive_attribute: str) -> float:        \n",
    "        global_dist = data[sensitive_attribute].value_counts(normalize=True) #global distribution of attribute\n",
    "        max_distance = 0        \n",
    "        groups = data.groupby(quasi_identifiers)\n",
    "        for _, group in groups:\n",
    "            group_dist = group[sensitive_attribute].value_counts(normalize=True)            \n",
    "            all_values = set(global_dist.index) | set(group_dist.index)\n",
    "            global_aligned = pd.Series([global_dist.get(v, 0) for v in all_values])\n",
    "            group_aligned = pd.Series([group_dist.get(v, 0) for v in all_values])            \n",
    "            #approximate Earth Mover's Distance to calculate difference in distributions\n",
    "            distance = np.abs(np.cumsum(global_aligned - group_aligned)).max()\n",
    "            max_distance = max(max_distance, distance)         \n",
    "        return max_distance\n",
    "\n",
    "    def differential_noise(self, data: pd.DataFrame, column: str, stddev: float) -> pd.Series:\n",
    "        true_counts = data[column].value_counts()\n",
    "        scale = stddev / self.epsilon\n",
    "        noise = np.random.normal(0, stddev, size=len(true_counts)) #normally distributed noise\n",
    "        noisy_counts = true_counts + noise\n",
    "        noisy_counts = noisy_counts.clip(lower=0) #values cannot be negative in our dataset        \n",
    "        return noisy_counts\n",
    "\n",
    "    def privacy_metrics(self, data: pd.DataFrame, quasi_identifiers: List[str], sensitive_attributes: List[str]) -> Dict:\n",
    "        metrics = {}\n",
    "        metrics['k_anonymity'] = self.k_anonymity(data, quasi_identifiers)\n",
    "        l_div_scores = []\n",
    "        for attr in sensitive_attributes:\n",
    "            l_div_scores.append(self.l_diversity(data, quasi_identifiers, attr))\n",
    "        metrics['l_diversity'] = min(l_div_scores)\n",
    "        t_close_scores = []\n",
    "        for attr in sensitive_attributes:\n",
    "            t_close_scores.append(self.t_closeness(data, quasi_identifiers, attr))\n",
    "        metrics['t_closeness'] = max(t_close_scores)\n",
    "        dp_counts = {}\n",
    "        for attr in sensitive_attributes:\n",
    "            dp_counts[attr] = self.differential_noise(data, attr, stddev=1.0)\n",
    "        metrics['dp_counts'] = dp_counts        \n",
    "        return metrics\n",
    "\n",
    "    #function to calculate overall privacy score using randomly chosen weights for each score\n",
    "    def privacy_score(self, metrics: Dict) -> float:\n",
    "        k_norm = min(metrics['k_anonymity'] / 3, 1.0)  \n",
    "        l_norm = min(metrics['l_diversity'] / 2, 1.0)  \n",
    "        t_norm = 1 - metrics['t_closeness'] \n",
    "        weights = {'k_anonymity': 0.4, 'l_diversity': 0.3, 't_closeness': 0.3}\n",
    "        privacy_score = (weights['k_anonymity'] * k_norm +weights['l_diversity'] * l_norm +weights['t_closeness'] * t_norm)        \n",
    "        return round(privacy_score, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to compare privacy of two datasets\n",
    "def compare_datasets(dataset1: pd.DataFrame, dataset2: pd.DataFrame, quasi_identifiers: List[str], sensitive_attributes: List[str], epsilon: float = 0.1) -> Dict:\n",
    "    analyzer = PrivacyMetrics(epsilon=epsilon)    \n",
    "    metrics1 = analyzer.privacy_metrics(dataset1, quasi_identifiers, sensitive_attributes)\n",
    "    metrics2 = analyzer.privacy_metrics(dataset2, quasi_identifiers, sensitive_attributes)\n",
    "    score1 = analyzer.privacy_score(metrics1)\n",
    "    score2 = analyzer.privacy_score(metrics2)    \n",
    "    return {\n",
    "        'dataset1': {'metrics': metrics1, 'privacy_score': score1},\n",
    "        'dataset2': {'metrics': metrics2, 'privacy_score': score2}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparision\n",
    "quasi_identifiers = ['age', 'pincode']\n",
    "sensitive_attributes = ['blood_pressure', 'condition']\n",
    "results = compare_datasets(dataset1, dataset2, quasi_identifiers, sensitive_attributes, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset 1 Analysis:\n",
      "------------------\n",
      "k-anonymity: 2\n",
      "l-diversity: 2\n",
      "t-closeness: 0.500\n",
      "Final Privacy Score: 0.717\n",
      "\n",
      "Differentially Private Counts:\n",
      "\n",
      "blood_pressure:\n",
      "blood_pressure\n",
      "high      4.964887\n",
      "normal    3.612012\n",
      "low       3.536350\n",
      "Name: count, dtype: float64\n",
      "\n",
      "condition:\n",
      "condition\n",
      "diabetes        3.440919\n",
      "flu             2.525616\n",
      "hypertension    2.505825\n",
      "asthma          3.978846\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Dataset 2 Analysis:\n",
      "------------------\n",
      "k-anonymity: 2\n",
      "l-diversity: 2\n",
      "t-closeness: 0.500\n",
      "Final Privacy Score: 0.717\n",
      "\n",
      "Differentially Private Counts:\n",
      "\n",
      "blood_pressure:\n",
      "blood_pressure\n",
      "high      4.845844\n",
      "normal    3.766208\n",
      "low       2.381761\n",
      "Name: count, dtype: float64\n",
      "\n",
      "condition:\n",
      "condition\n",
      "diabetes        3.953843\n",
      "flu             1.061946\n",
      "asthma          1.391328\n",
      "hypertension    3.470046\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Results\n",
    "print(\"\\nDataset 1 Analysis:\")\n",
    "print(\"------------------\")\n",
    "print(f\"k-anonymity: {results['dataset1']['metrics']['k_anonymity']}\")\n",
    "print(f\"l-diversity: {results['dataset1']['metrics']['l_diversity']}\")\n",
    "print(f\"t-closeness: {results['dataset1']['metrics']['t_closeness']:.3f}\")\n",
    "print(f\"Final Privacy Score: {results['dataset1']['privacy_score']}\")\n",
    "print(\"\\nDifferentially Private Counts:\")\n",
    "for attr in sensitive_attributes:\n",
    "    print(f\"\\n{attr}:\")\n",
    "    print(results['dataset1']['metrics']['dp_counts'][attr])\n",
    "\n",
    "print(\"\\nDataset 2 Analysis:\")\n",
    "print(\"------------------\")\n",
    "print(f\"k-anonymity: {results['dataset2']['metrics']['k_anonymity']}\")\n",
    "print(f\"l-diversity: {results['dataset2']['metrics']['l_diversity']}\")\n",
    "print(f\"t-closeness: {results['dataset2']['metrics']['t_closeness']:.3f}\")\n",
    "print(f\"Final Privacy Score: {results['dataset2']['privacy_score']}\")\n",
    "print(\"\\nDifferentially Private Counts:\")\n",
    "for attr in sensitive_attributes:\n",
    "    print(f\"\\n{attr}:\")\n",
    "    print(results['dataset2']['metrics']['dp_counts'][attr])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
